{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE9DUv8MzyG1",
        "outputId": "a82e78fd-7388-4571-8bc9-1839103db2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.6)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.11.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.7.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install Libs\n",
        "!pip install mediapipe\n",
        "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "!pip install mlxtend\n",
        "!unzip -p \"#AZAD_Lab_MH-FED_2022#\" /content/gdrive/MyDrive/MH-FED_Images.zip\n",
        "\n",
        "# Mount Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation Tools\n",
        "\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "  face_landmarks_list = detection_result.face_landmarks\n",
        "  annotated_image = np.copy(rgb_image)\n",
        "\n",
        "  # Loop through the detected faces to visualize.\n",
        "  for idx in range(len(face_landmarks_list)):\n",
        "    face_landmarks = face_landmarks_list[idx]\n",
        "\n",
        "    # Draw the face landmarks.\n",
        "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "    face_landmarks_proto.landmark.extend([\n",
        "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
        "    ])\n",
        "\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_tesselation_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
        "        landmark_drawing_spec=None,\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "        .get_default_face_mesh_contours_style())\n",
        "    solutions.drawing_utils.draw_landmarks(\n",
        "        image=annotated_image,\n",
        "        landmark_list=face_landmarks_proto,\n",
        "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
        "          landmark_drawing_spec=None,\n",
        "          connection_drawing_spec=mp.solutions.drawing_styles\n",
        "          .get_default_face_mesh_iris_connections_style())\n",
        "\n",
        "  return annotated_image\n",
        "\n",
        "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
        "  # Extract the face blendshapes category names and scores.\n",
        "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
        "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
        "  # The blendshapes are ordered in decreasing score value.\n",
        "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 12))\n",
        "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
        "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
        "  ax.invert_yaxis()\n",
        "\n",
        "  # Label each bar with values\n",
        "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
        "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
        "\n",
        "  ax.set_xlabel('Score')\n",
        "  ax.set_title(\"Face Blendshapes\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nbgJvEQYz4LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "def show_image(image_path):\n",
        "  img = cv2.imread(image_path)\n",
        "  cv2_imshow(img)\n",
        "\n",
        "# show_image(\"/content/gdrive/MyDrive/images/1/Anger.jpg\")"
      ],
      "metadata": {
        "id": "WPGS6IGSz7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "def get_features(image_path, show_visuals = False):\n",
        "# Create an FaceLandmarker object.\n",
        "  base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
        "  options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
        "                                        output_face_blendshapes=True,\n",
        "                                        output_facial_transformation_matrixes=True,\n",
        "                                        num_faces=1)\n",
        "  detector = vision.FaceLandmarker.create_from_options(options)\n",
        "  image = mp.Image.create_from_file(image_path)\n",
        "  detection_result = detector.detect(image)\n",
        "\n",
        "  if show_visuals:\n",
        "    annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "    show_image(image_path)\n",
        "    cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
        "  return detection_result\n",
        "\n",
        "def get_major_features(result, threshold, add_label = False, label = None):\n",
        "  features_list = result.face_blendshapes[0]\n",
        "  # for feature in features_list:\n",
        "  #   print(feature)\n",
        "  low = min(features_list, key=lambda x: x.score).score\n",
        "  high = max(features_list, key=lambda x: x.score).score\n",
        "  n = high - low\n",
        "  # print(low, high)\n",
        "  out = [feature.category_name for feature in features_list if (feature.score - low) / n >= threshold]\n",
        "  if add_label:\n",
        "    out.append(label)\n",
        "  # for feature in features_list:\n",
        "  #   print(feature.category_name, ((feature.score - low) / n), (feature.score - low) / n >= threshold)\n",
        "  return out\n",
        "\n",
        "def get_dataset_features(images_list, threshold, add_label = False, label = None):\n",
        "  out = []\n",
        "  for i, image in enumerate(images_list):\n",
        "    print(f\"{image}\")\n",
        "    result = get_features(image)\n",
        "    out.append(get_major_features(result, threshold, add_label, label))\n",
        "  return out\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "def apriori(features, min_support=0.5, min_confidence=0.5):\n",
        "  init = defaultdict(int)\n",
        "  for item in features:\n",
        "    for feature in item:\n",
        "      init[feature] += 1\n",
        "  support_value = min_support * sum(init.values())\n",
        "\n",
        "  return init\n",
        "\n",
        "def a2(data, min_support, min_confidence):\n",
        "  rules = {}\n",
        "  init = []\n",
        "  for i in data:\n",
        "      for q in i:\n",
        "          if(q not in init):\n",
        "              init.append(q)\n",
        "  init = sorted(init)\n",
        "  # print(init)\n",
        "\n",
        "  c = Counter()\n",
        "  for i in init:\n",
        "      for d in data:\n",
        "          if(i in d):\n",
        "              c[i]+=1\n",
        "  # print(\"C1:\")\n",
        "  # for i in c:\n",
        "  #     print(str([i])+\": \"+str(c[i]))\n",
        "  # print()\n",
        "  s = int(min_support*len(init))\n",
        "  l = Counter()\n",
        "  for i in c:\n",
        "      if(c[i] >= s):\n",
        "          l[frozenset([i])]+=c[i]\n",
        "  # print(\"L1:\")\n",
        "  # for i in l:\n",
        "  #     print(str(list(i))+\": \"+str(l[i]))\n",
        "  # print()\n",
        "  pl = l\n",
        "  pos = 1\n",
        "  for count in range (2,1000):\n",
        "      nc = set()\n",
        "      temp = list(l)\n",
        "      for i in range(0,len(temp)):\n",
        "          for j in range(i+1,len(temp)):\n",
        "              t = temp[i].union(temp[j])\n",
        "              if(len(t) == count):\n",
        "                  nc.add(temp[i].union(temp[j]))\n",
        "      nc = list(nc)\n",
        "      c = Counter()\n",
        "      for i in nc:\n",
        "          c[i] = 0\n",
        "          for q in data:\n",
        "              temp = set(q)\n",
        "              if(i.issubset(temp)):\n",
        "                  c[i]+=1\n",
        "      # print(\"C\"+str(count)+\":\")\n",
        "      # for i in c:\n",
        "      #     print(str(list(i))+\": \"+str(c[i]))\n",
        "      # print()\n",
        "      l = Counter()\n",
        "      for i in c:\n",
        "          if(c[i] >= s):\n",
        "              l[i]+=c[i]\n",
        "      # print(\"L\"+str(count)+\":\")\n",
        "      # for i in l:\n",
        "      #     print(str(list(i))+\": \"+str(l[i]))\n",
        "      # print()\n",
        "      if(len(l) == 0):\n",
        "          break\n",
        "      rules.update(get_rules(get_association_rules(l, data), min_confidence))\n",
        "      pl = l\n",
        "      pos = count\n",
        "  # print(\"Result: \")\n",
        "  # print(\"L\"+str(pos)+\":\")\n",
        "  # for i in pl:\n",
        "      # print(str(list(i))+\": \"+str(pl[i]))\n",
        "  # print()\n",
        "  for l in pl:\n",
        "      c = [frozenset(q) for q in combinations(l,len(l)-1)]\n",
        "      mmax = 0\n",
        "      for a in c:\n",
        "          b = l-a\n",
        "          ab = l\n",
        "          sab = 0\n",
        "          sa = 0\n",
        "          sb = 0\n",
        "          for q in data:\n",
        "              temp = set(q)\n",
        "              if(a.issubset(temp)):\n",
        "                  sa+=1\n",
        "              if(b.issubset(temp)):\n",
        "                  sb+=1\n",
        "              if(ab.issubset(temp)):\n",
        "                  sab+=1\n",
        "          temp = sab/sa*100\n",
        "          if(temp > mmax):\n",
        "              mmax = temp\n",
        "          temp = sab/sb*100\n",
        "          if(temp > mmax):\n",
        "              mmax = temp\n",
        "          # print(str(list(a))+\" -> \"+str(list(b))+\" = \"+str(sab/sa*100)+\"%\")\n",
        "          # print(str(list(b))+\" -> \"+str(list(a))+\" = \"+str(sab/sb*100)+\"%\")\n",
        "      curr = 1\n",
        "      # print(\"choosing:\", end=' ')\n",
        "      for a in c:\n",
        "          b = l-a\n",
        "          ab = l\n",
        "          sab = 0\n",
        "          sa = 0\n",
        "          sb = 0\n",
        "          for q in data:\n",
        "              temp = set(q)\n",
        "              if(a.issubset(temp)):\n",
        "                  sa+=1\n",
        "              if(b.issubset(temp)):\n",
        "                  sb+=1\n",
        "              if(ab.issubset(temp)):\n",
        "                  sab+=1\n",
        "          temp = sab/sa*100\n",
        "          # if(temp == mmax):\n",
        "              # print(curr, end = ' ')\n",
        "          curr += 1\n",
        "          temp = sab/sb*100\n",
        "          # if(temp == mmax):\n",
        "              # print(curr, end = ' ')\n",
        "          curr += 1\n",
        "      # print()\n",
        "      # print()\n",
        "  itemset = []\n",
        "  for key in pl.keys():\n",
        "    itemset.append(key)\n",
        "  return itemset, rules\n",
        "\n",
        "def get_association_rules(l_set, data):\n",
        "  out = \"\"\n",
        "  for l in l_set:\n",
        "      c = [frozenset(q) for q in combinations(l,len(l)-1)]\n",
        "      mmax = 0\n",
        "      for a in c:\n",
        "          b = l-a\n",
        "          ab = l\n",
        "          sab = 0\n",
        "          sa = 0\n",
        "          sb = 0\n",
        "          for q in data:\n",
        "              temp = set(q)\n",
        "              if(a.issubset(temp)):\n",
        "                  sa+=1\n",
        "              if(b.issubset(temp)):\n",
        "                  sb+=1\n",
        "              if(ab.issubset(temp)):\n",
        "                  sab+=1\n",
        "          temp = sab/sa*100\n",
        "          if(temp > mmax):\n",
        "              mmax = temp\n",
        "          temp = sab/sb*100\n",
        "          if(temp > mmax):\n",
        "              mmax = temp\n",
        "          out += str(list(a))+\" -> \"+str(list(b))+\" = \"+str(sab/sa*100)+\"%\\n\"\n",
        "          out += str(list(b))+\" -> \"+str(list(a))+\" = \"+str(sab/sb*100)+\"%\\n\"\n",
        "  return out\n",
        "\n",
        "def breakdown_rule(rule, threshold):\n",
        "  rule.strip()\n",
        "  if rule == \"\":\n",
        "    return None, None\n",
        "  left, right = rule.split(\"->\")\n",
        "  right, confidence = right.split(\"=\")\n",
        "  confidence = float(confidence.strip()[:-1])\n",
        "  if confidence < (threshold * 100):\n",
        "    return None, None\n",
        "  left = left.strip()[1:-1]\n",
        "  left = [x.strip()[1:-1] for x in left.split(\",\")]\n",
        "  right = right.strip()[1:-1]\n",
        "  right = [x.strip()[1:-1] for x in right.split(\",\")]\n",
        "  return left, right\n",
        "def get_rules(text, confidence):\n",
        "  rules = defaultdict(set)\n",
        "  rules_text = text.split(\"\\n\")\n",
        "  for rule in rules_text:\n",
        "    left, right = breakdown_rule(rule, confidence)\n",
        "    if left is None or right is None:\n",
        "      continue\n",
        "    rules[frozenset(left)].update(right)\n",
        "  return rules"
      ],
      "metadata": {
        "id": "aQdD0Fnhz_NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_output(itemset, rules):\n",
        "  for i, item in enumerate(itemset):\n",
        "    itemset[i] = set(item)\n",
        "  rule_list = []\n",
        "  for k, v in rules.items():\n",
        "    rule_list.append((set(k), set(v)))\n",
        "  rule_list.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "  return itemset, rule_list"
      ],
      "metadata": {
        "id": "mt3qdrA50MoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_ACCEPT_THRESHOLD = 0.7\n",
        "APRIORI_SUPPORT_THRESHOLD = 0.5\n",
        "APRIORI_CONFIDENCE_THRESHOLD = 1.0\n",
        "labels = [\"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
        "\n",
        "class Emotion:\n",
        "  def __init__(self, itemset, rules, label):\n",
        "    self.itemset = itemset\n",
        "    self.rules = rules\n",
        "    self.label = label\n",
        "\n",
        "emotion_outputs = {}\n",
        "for label in labels:\n",
        "  features = get_dataset_features([f\"/content/gdrive/MyDrive/images/{i}/{label}.jpg\" for i in range(19)], FEATURE_ACCEPT_THRESHOLD, False, label)\n",
        "  itemset, rules = a2(features, APRIORI_SUPPORT_THRESHOLD, APRIORI_CONFIDENCE_THRESHOLD)\n",
        "  itemset, rules = clean_output(itemset, rules)\n",
        "  emotion_outputs[label] = Emotion(itemset, rules, label)\n",
        "\n",
        "for label in labels:\n",
        "  print(f\"Emotion - {label}\")\n",
        "  print(\"Itemsets\")\n",
        "  for itemset in emotion_outputs[label].itemset:\n",
        "    print(itemset)\n",
        "  print()\n",
        "  for k, v in emotion_outputs[label].rules:\n",
        "    print(f\"{k} -> {v}\")\n",
        "  print(\"----------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "bJTxBkes0P0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_features(features, emotion):\n",
        "  out = features\n",
        "  for k, v in emotion_outputs[emotion].rules:\n",
        "    if k.issubset(out):\n",
        "      out.update(v)\n",
        "  return out\n",
        "\n",
        "def test_image(image_path):\n",
        "  result = get_features(image_path)\n",
        "  features = set(get_major_features(result, APRIORI_SUPPORT_THRESHOLD))\n",
        "  out = []\n",
        "  for label in labels:\n",
        "    extended_features = extend_features(features, label)\n",
        "    for model in emotion_outputs[label].itemset:\n",
        "      if model.issubset(extended_features):\n",
        "        out.append(label)\n",
        "        break\n",
        "  return out\n",
        "\n",
        "print(test_image(\"/content/gdrive/MyDrive/s_sad.jpeg\"))"
      ],
      "metadata": {
        "id": "1io-maY20S68"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}